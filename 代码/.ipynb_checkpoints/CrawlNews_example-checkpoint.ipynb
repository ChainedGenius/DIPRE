{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_news_list(root, url, date_boundary):\n",
    "    article_list = []\n",
    "    while 1:\n",
    "        path = root + url\n",
    "        html = requests.get(path)\n",
    "        soup = BeautifulSoup(html.text)\n",
    "        body = soup.find('ul', 'wp_article_list')  # 寻找对应的新闻列表\n",
    "        for item in body.find_all('li'):  # 遍历新闻列表中的每条新闻组\n",
    "            date = item.find('span', 'Article_PublishDate').string  # 通过每条新闻组中 \tspan的class获取时间字符串（不用.string获取的为tag）\n",
    "            convert_date = time.mktime(time.strptime(date, \"%Y-%m-%d\"))  # 用秒数来表示时间\n",
    "            if convert_date < date_boundary:\n",
    "                break\n",
    "            title = item.find('a')['title']\n",
    "            href = item.find('a')['href']\n",
    "            article_list.append([date, title, href])  # 将数据追加保存到article_list中\n",
    "        # print(title, href ,date)\n",
    "        if time.mktime(time.strptime(date, '%Y-%m-%d')) > date_boundary:\n",
    "            next_page = soup.find('ul', 'wp_paging clearfix')\n",
    "            url = next_page.find('a', {'class': 'next'})['href']\n",
    "        else:\n",
    "            break\n",
    "    #print(article_list)\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news(root,article_list):\n",
    "    news_dict = {}\n",
    "    i = 1\n",
    "    for item in article_list:\n",
    "        date,title,href = item # 将数组中的三个元素分别赋值给变量\n",
    "        #print(date,title,href)\n",
    "        if 'http' in href:\n",
    "            path = href\n",
    "        else:\n",
    "            path = root + href\n",
    "        f = requests.get(path)\n",
    "        soup = BeautifulSoup(f.text)\n",
    "        body = soup.find('div',{'class','acd'})\n",
    "        content = body.find('div',{'class','wp_articlecontent'})\n",
    "        text = u''\n",
    "        for a in content.strings: # 效果等同于text = content.text\n",
    "            text += a\n",
    "        \n",
    "        publis = body.find_all('span','arti_publisher')[1].string\n",
    "        department = publis.split(u'\\uff1a')[1] # 将publis通过 ':'进行分割，效果等同于 publis.split(':')\n",
    "        if department == '':\n",
    "            department = 'none'\n",
    "        view_times = body.find('span',['class','WP_VisitCount']).text\n",
    "        #逐个把新闻信息加入字典\n",
    "        news_dict[i] = {'date':date,'title':title,'source':department,'content':text,'views':view_times,'url':path}\n",
    "        \n",
    "        time.sleep(0.1) # 控制爬虫速度\n",
    "        i += 1;\n",
    "    return news_dict\n",
    "\n",
    "def search(keywords,news_dict):\n",
    "    result = {} # result为字典数据类型\n",
    "    title_list = [] # 列表数据类型\n",
    "    source_list = []\n",
    "    content_list = []\n",
    "    for x in news_dict: # 对于字典数据类型，x的值为1、2、3......\n",
    "        if keywords in news_dict[x]['title']:\n",
    "            title_list.append([x,news_dict[x]['date'],news_dict[x]['title']])\n",
    "        if keywords in news_dict[x]['source']:\n",
    "            source_list.append([x,news_dict[x]['date'],news_dict[x]['source']])\n",
    "        if keywords in news_dict[x]['content']:\n",
    "            content_list.append([x,news_dict[x]['date'],news_dict[x]['content']])\n",
    "    \n",
    "    result['title'] = title_list\n",
    "    result['source'] = source_list\n",
    "    result['content_list'] = content_list\n",
    "    return result\n",
    "        # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # 如果这个module文件是被当成程序来执行，那么，该__name__属性的值就是\"__main__\"\n",
    "    root = \"http://www.suibe.edu.cn/_s19\"\n",
    "    url = \"/1416/list.psp\"\n",
    "    date_boundary = time.mktime(time.strptime(\"2019-08-01\", \"%Y-%m-%d\"))\n",
    "    news_list = get_news_list(root, url, date_boundary)\n",
    "    news_dict = crawl_news(root,news_list)\n",
    "    result = search(u'上海对外经贸大学专场',news_dict)\n",
    "    for x in result['title']:\n",
    "        print(x[0],x[1])\n",
    "        print(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search(u'上海对外经贸大学专场',news_dict)\n",
    "for x in result['title']:\n",
    "    print(x[0],x[1])\n",
    "    print(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
